{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca770702",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Object detection using Transfer Learning of CNN architectures\n",
    "a. Load in a pre-trained CNN model trained on a large dataset\n",
    "b. Freeze parameters (weights) in model’s lower convolutional layers\n",
    "c. Add custom classifier with several layers of trainable parameters to model\n",
    "d. Train classifier layers on training data available for task\n",
    "e. Fine-tune hyper parameters and unfreeze more layers as needed\n",
    "\n",
    "dataset: caltech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d6cb61-6448-4075-8227-d54bf63146eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0d824f-ed21-4292-ab9d-abbc05c0a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9144 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"caltech-101-img/\"\n",
    "dataset_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    ")\n",
    "\n",
    "# here batch_size is the number of images in each batch\n",
    "batch_size = 2000\n",
    "dataset_generator = dataset_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f95c82-aad6-4c75-ab50-b9802f810d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train =  dataset_generator[0]\n",
    "x_test, y_test = dataset_generator[1]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b334c1d-6ee2-41c4-8856-80931bac0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 without top layers\n",
    "weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfda76f3-3281-495f-95f6-c682846f0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f336276-ef37-420f-a6c5-444b2be86503",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6779eb96-b203-4a5f-8868-78f249004106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.6110 - loss: 1.8081 - val_accuracy: 0.5210 - val_loss: 2.1999\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.6570 - loss: 1.5603 - val_accuracy: 0.5350 - val_loss: 2.0915\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.7015 - loss: 1.3757 - val_accuracy: 0.5435 - val_loss: 1.9996\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.7535 - loss: 1.1984 - val_accuracy: 0.5530 - val_loss: 1.9481\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.7845 - loss: 1.0639 - val_accuracy: 0.5585 - val_loss: 1.8904\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8045 - loss: 0.9481 - val_accuracy: 0.5690 - val_loss: 1.8723\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8320 - loss: 0.8454 - val_accuracy: 0.5710 - val_loss: 1.8489\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8505 - loss: 0.7658 - val_accuracy: 0.5865 - val_loss: 1.8136\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8825 - loss: 0.6732 - val_accuracy: 0.5800 - val_loss: 1.7909\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9000 - loss: 0.5972 - val_accuracy: 0.5920 - val_loss: 1.7695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29d350abed0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a6778-1867-4be3-8ffe-3138d8d38195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 353ms/step"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predicted_value = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669345e-3525-4071-baa5-5ef73596f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
